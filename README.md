# AnÃ¡lisis de Sentimiento en Tweets con RNN, LSTM y BiLSTM + AtenciÃ³n

Este proyecto implementa y compara diferentes arquitecturas de redes neuronales recurrentes para la clasificaciÃ³n de sentimientos en tweets.


## Ãndice
1. [Arquitecturas utilizadas](#arquitecturas-utilizadas)
   - [Â¿QuÃ© es una RNN?](#quÃ©-es-una-rnn)
   - [Â¿QuÃ© es una LSTM?](#quÃ©-es-una-lstm)
   - [Â¿QuÃ© es una BiLSTM con atenciÃ³n?](#quÃ©-es-una-bilstm-con-atenciÃ³n)
2. [IntroducciÃ³n a la problemÃ¡tica](#introducciÃ³n-a-la-problemÃ¡tica)
3. [ExploraciÃ³n del dataset](#exploraciÃ³n-del-dataset)
   - [DescripciÃ³n de las variables](#descripciÃ³n-de-las-variables)
   - [AnÃ¡lisis de caracterÃ­sticas y preprocesamiento](#anÃ¡lisis-de-las-caracterÃ­sticas-y-preprocesamiento-de-datos)
4. [ImplementaciÃ³n de la red neuronal](#implementaciÃ³n-de-la-red-neuronal)
5. [EstructuraciÃ³n del repositorio](#estructuraciÃ³n-del-repositorio)
6. [Instrucciones de uso](#instrucciones-de-uso-de-la-red-neuronal-y-rendimiento-obtenido)
   - [Requisitos previos](#requisitos-previos)
   - [InstalaciÃ³n](#instalaciÃ³n)
   - [CÃ³mo entrenar el modelo](#cÃ³mo-entrenar-el-modelo-desde-trainpy)
   - [CÃ³mo evaluar el modelo](#cÃ³mo-evaluar-el-modelo-usando-evaluatepy)
   - [CÃ³mo hacer predicciones](#cÃ³mo-hacer-predicciones-con-predictpy)
7. [Rendimiento obtenido](#rendimiento-obtenido)
8. [Conclusiones](#conclusiones)


## âš¡ï¸ Arquitecturas utilizadas

En este proyecto se implementaron y compararon tres tipos de arquitecturas de redes neuronales recurrentes para el anÃ¡lisis de sentimientos en Tweets:

### Â¿QuÃ© es una RNN?

![Red Neuronal Recurrente](https://www.researchgate.net/profile/Rishikesh-Gawde/publication/351840108/figure/fig1/AS:1027303769374723@1621939713840/Fig-3-RNN-A-recurrent-neural-network-RNN-is-a-class-of-artificial-neural-networks.ppm)

Una Red Neuronal Recurrente (RNN) es un tipo de red diseÃ±ada para procesar secuencias de datos. A diferencia de una red neuronal tradicional, una RNN tiene una "memoria interna" que le permite recordar informaciÃ³n de entradas anteriores, lo cual es Ãºtil en tareas como procesamiento de lenguaje natural. Sin embargo, las RNN simples suelen sufrir del problema de desvanecimiento del gradiente, lo que limita su capacidad para aprender dependencias a largo plazo.

### Â¿QuÃ© es una LSTM?

![Red Neuronal LSTM](https://mlarchive.com/wp-content/uploads/2024/04/New-Project-3-1-1024x607-1024x585.png)
La Long Short-Term Memory (LSTM) es una variante de la RNN diseÃ±ada para resolver los problemas de memoria de largo plazo. Utiliza compuertas (gate mechanisms) que controlan el flujo de informaciÃ³n, permitiendo al modelo "recordar" o "olvidar" informaciÃ³n segÃºn sea necesario. Esto la hace mucho mÃ¡s efectiva para tareas como clasificaciÃ³n de sentimientos en texto.


### Â¿QuÃ© es una BiLSTM con atenciÃ³n?

![BILSTM + ATTENTION](https://www.researchgate.net/profile/Hao-Wu-19/publication/329512919/figure/fig1/AS:701588624646144@1544283171783/The-architecture-of-BiLSTM-Attention-model.ppm)

Una BiLSTM (Bidirectional LSTM) procesa la secuencia tanto hacia adelante como hacia atrÃ¡s, capturando contexto tanto previo como posterior en una oraciÃ³n. Esto mejora significativamente la comprensiÃ³n semÃ¡ntica. AdemÃ¡s, al aÃ±adir un mecanismo de **atenciÃ³n**, el modelo puede aprender a enfocarse en las palabras mÃ¡s relevantes del texto para tomar decisiones de clasificaciÃ³n, mejorando la interpretaciÃ³n y el rendimiento en tareas complejas como el anÃ¡lisis de sentimiento.

## âš¡ï¸ IntroducciÃ³n a la problemÃ¡tica

### Aplicaciones del anÃ¡lisis de sentimiento en redes sociales

El **anÃ¡lisis de sentimiento** es una tÃ©cnica clave del procesamiento de lenguaje natural (NLP) que busca identificar y extraer opiniones, emociones o actitudes expresadas en texto. En el contexto de redes sociales como Twitter, esta tÃ©cnica tiene aplicaciones muy valiosas:

- **Monitoreo de la opiniÃ³n pÃºblica**: detectar cÃ³mo reaccionan los usuarios ante eventos, marcas o personalidades.
- **GestiÃ³n de reputaciÃ³n**: las empresas pueden actuar rÃ¡pidamente ante comentarios negativos o detectar embajadores de marca.
- **Marketing dirigido**: segmentaciÃ³n de usuarios segÃºn sus actitudes o emociones.
- **Sistemas de recomendaciÃ³n**: ofrecer contenido, productos o servicios personalizados en funciÃ³n del sentimiento expresado.
- **AnÃ¡lisis polÃ­tico o social**: comprender el sentimiento colectivo ante decisiones polÃ­ticas o eventos sociales.

### Importancia de modelar secuencias y contexto lingÃ¼Ã­stico

A diferencia de tareas de clasificaciÃ³n mÃ¡s simples, en el anÃ¡lisis de sentimiento **el orden de las palabras y su contexto importan enormemente**. Por ejemplo:

> *"I don't like this"* no es lo mismo que *"I like this"*

Modelos como las RNN, LSTM y BiLSTM son esenciales porque:
- Permiten modelar **dependencias temporales** entre palabras.
- Capturan **el contexto lingÃ¼Ã­stico**, incluyendo negaciones, ironÃ­as o emociones implÃ­citas.
- En el caso de BiLSTM, permiten analizar un texto **en ambas direcciones** (inicio â†’ fin y fin â†’ inicio).
- El mecanismo de **atenciÃ³n** mejora la capacidad del modelo para "enfocarse" en las palabras mÃ¡s relevantes dentro del tweet.

---

## âš¡ï¸ ExploraciÃ³n del dataset

Para entrenar nuestros modelos, trabajamos con un conjunto de datos de tweets pÃºblicos extraÃ­do de [este repositorio de GitHub](https://raw.githubusercontent.com/dD2405/Twitter_Sentiment_Analysis/master/train.csv). El dataset contiene **31,962 tweets en inglÃ©s**, cada uno etiquetado con su **sentimiento binario**: positivo o negativo.

### **DescripciÃ³n de las variables**

| Columna | DescripciÃ³n |
|---------|-------------|
| `id`    | Identificador Ãºnico del tweet. |
| `label` | Sentimiento asociado al tweet (`0`: negativo, `1`: positivo). |
| `tweet` | Texto completo del tweet, que puede contener menciones, hashtags, emojis, URLs y sÃ­mbolos. |


### **Preprocesamiento aplicado al dataset**



## Estructura del Repositorio

```
mi_proyecto_sentimiento/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploracion.ipynb         # ExploraciÃ³n y preprocesamiento de datos
â”‚   â”œâ”€â”€ 02_entrenamiento_RNN.ipynb   # Entrenamiento y evaluaciÃ³n del modelo RNN
â”‚   â”œâ”€â”€ 03_entrenamiento_LSTM.ipynb  # Entrenamiento y evaluaciÃ³n del modelo LSTM
â”‚   â””â”€â”€ 04_BiLSTM_atencion.ipynb     # Entrenamiento y evaluaciÃ³n del modelo BiLSTM+AtenciÃ³n
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py               # Funciones para cargar y preprocesar los datos
â”‚   â”œâ”€â”€ model_rnn.py                 # ImplementaciÃ³n del modelo RNN
â”‚   â”œâ”€â”€ model_lstm.py                # ImplementaciÃ³n del modelo LSTM
â”‚   â”œâ”€â”€ model_bilstm_attention.py    # ImplementaciÃ³n del modelo BiLSTM+AtenciÃ³n
â”‚   â”œâ”€â”€ train.py                     # Funciones de entrenamiento
â”‚   â”œâ”€â”€ evaluate.py                  # Funciones de evaluaciÃ³n
â”‚   â””â”€â”€ utils.py                     # Funciones auxiliares
â”œâ”€â”€ models/                          # Directorio para guardar modelos entrenados
â”‚   â”œâ”€â”€ rnn_model.h5
â”‚   â”œâ”€â”€ lstm_model.h5
â”‚   â”œâ”€â”€ bilstm_attention_model.h5
â”‚   â””â”€â”€ README.md                    # DescripciÃ³n de los modelos guardados
â”œâ”€â”€ requirements.txt                 # Dependencias del proyecto
â””â”€â”€ .gitignore                       # Archivos a ignorar en Git
```

## Requisitos y Dependencias

Para ejecutar este proyecto, necesitas tener instalado Python 3.8 o superior y las siguientes bibliotecas:

```
pip install -r requirements.txt
```

## Uso

### Carga y ExploraciÃ³n de Datos

El dataset se carga directamente desde una URL pÃºblica:

```python
import pandas as pd
import tensorflow as tf

url = "https://raw.githubusercontent.com/dD2405/Twitter_Sentiment_Analysis/master/train.csv"
csv_path = tf.keras.utils.get_file("twitter_sentiment.csv", url)
df = pd.read_csv(csv_path)
```

### Entrenamiento de Modelos

Cada modelo se puede entrenar ejecutando el notebook correspondiente:

```bash
jupyter notebook notebooks/02_entrenamiento_RNN.ipynb
jupyter notebook notebooks/03_entrenamiento_LSTM.ipynb
jupyter notebook notebooks/04_BiLSTM_atencion.ipynb
```

TambiÃ©n se puede ejecutar desde la lÃ­nea de comandos usando los scripts en `src/`:

```bash
python src/train.py --model rnn --epochs 5 --batch_size 128
python src/train.py --model lstm --epochs 5 --batch_size 128
python src/train.py --model bilstm_attention --epochs 5 --batch_size 128
```

### EvaluaciÃ³n de Modelos

Para evaluar un modelo entrenado:

```bash
python src/evaluate.py --model models/bilstm_attention_model.h5 --test_size 0.2
```

## Arquitectura de los Modelos

### RNN BÃ¡sica
- Embedding Layer
- SimpleRNN Layer
- Dense Layer con activaciÃ³n sigmoid

### LSTM EstÃ¡ndar
- Embedding Layer
- LSTM Layer
- Dense Layer con activaciÃ³n sigmoid

### BiLSTM + AtenciÃ³n
- Embedding Layer
- Bidirectional LSTM con retorno de secuencias
- Capa de AtenciÃ³n personalizada
- Dense Layer con activaciÃ³n sigmoid

## Resultados

A continuaciÃ³n se presentan las mÃ©tricas de rendimiento de cada modelo:

| Modelo | Accuracy | Precision | Recall | F1-Score |
|--------|----------|-----------|--------|----------|
| RNN    | X.XX     | X.XX      | X.XX   | X.XX     |
| LSTM   | X.XX     | X.XX      | X.XX   | X.XX     |
| BiLSTM+AtenciÃ³n | X.XX | X.XX  | X.XX   | X.XX     |

### AnÃ¡lisis Comparativo

(AquÃ­ irÃ¡ un anÃ¡lisis comparativo de los resultados una vez entrenados los modelos)

## Manejo del Desbalanceo de Clases

Se implementaron dos estrategias para abordar el desbalanceo de clases (29720 negativos vs 2242 positivos):

1. **Oversampling**: DuplicaciÃ³n aleatoria de ejemplos de la clase minoritaria.
2. **SMOTE**: GeneraciÃ³n de ejemplos sintÃ©ticos para la clase minoritaria.

Los resultados muestran que... (Esto se completarÃ¡ con los resultados de los experimentos)



## âš¡ï¸ Rendimiento obtenido



## ğŸ§ Conclusiones




## Autores del Proyecto ğŸ¤“

Este proyecto fue desarrollado por:

* [JeremÃ­as PabÃ³n](https://github.com/jeremiaspabon) 
* [GersÃ³n JuliÃ¡n RincÃ³n](https://github.com/Julk-ui) 
* [AndrÃ©s Bravo](https://github.com/pipebravo10) 
* [Carolina Tobaria](https://github.com/Carot2) 