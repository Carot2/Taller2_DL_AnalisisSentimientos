{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de LSTM para Análisis de Sentimiento en Tweets\n",
    "\n",
    "Este notebook implementa y entrena un modelo LSTM (Long Short-Term Memory) para clasificar tweets según su sentimiento (positivo o negativo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuración Inicial e Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Añadir directorio de nivel superior al path para poder importar módulos del proyecto\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from src.utils import clean_text, plot_history, evaluate_model, plot_class_distribution\n",
    "from src.data_loader import load_data, prepare_data, get_class_weights\n",
    "from src.model_lstm import create_lstm_model\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set(style=\"whitegrid\", palette=\"deep\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Configurar memoria de GPU (si está disponible)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPUs disponibles: {len(gpus)}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga y Preparación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "MAX_WORDS = 10000  # Tamaño máximo del vocabulario\n",
    "MAX_LEN = 100      # Longitud máxima de las secuencias\n",
    "TEST_SIZE = 0.2    # Proporción para conjunto de prueba\n",
    "RANDOM_STATE = 42  # Semilla para reproducibilidad\n",
    "BATCH_SIZE = 128   # Tamaño del batch para entrenamiento\n",
    "EPOCHS = 5         # Número de épocas de entrenamiento\n",
    "\n",
    "# Estrategia para el desbalanceo de clases\n",
    "# Opciones: None, 'oversampling', 'smote', 'class_weights'\n",
    "BALANCE_STRATEGY = 'oversampling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "df = load_data(balance_method='oversampling' if BALANCE_STRATEGY == 'oversampling' else None)\n",
    "\n",
    "# Mostrar la distribución de clases\n",
    "plot_class_distribution(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para entrenamiento\n",
    "X_train, X_test, y_train, y_test, tokenizer = prepare_data(\n",
    "    df, \n",
    "    max_words=MAX_WORDS, \n",
    "    max_len=MAX_LEN, \n",
    "    test_size=TEST_SIZE, \n",
    "    balance_method='smote' if BALANCE_STRATEGY == 'smote' else None,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Información sobre los datos preparados\n",
    "print(f\"Tamaño del vocabulario: {min(len(tokenizer.word_index) + 1, MAX_WORDS)}\")\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {X_train.shape}\")\n",
    "print(f\"Tamaño del conjunto de prueba: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Construcción y Compilación del Modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros del modelo\n",
    "VOCAB_SIZE = min(len(tokenizer.word_index) + 1, MAX_WORDS)\n",
    "EMBEDDING_DIM = 64   # Dimensión de los embeddings\n",
    "LSTM_UNITS = 64      # Número de unidades en la capa LSTM\n",
    "DROPOUT_RATE = 0.3   # Tasa de dropout para regularización\n",
    "\n",
    "# Crear modelo LSTM\n",
    "model = create_lstm_model(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    lstm_units=LSTM_UNITS,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "\n",
    "# Mostrar resumen del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    ModelCheckpoint(\n",
    "        filepath='../models/lstm_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Calcular class weights si es necesario\n",
    "class_weights = get_class_weights(y_train) if BALANCE_STRATEGY == 'class_weights' else None\n",
    "if BALANCE_STRATEGY == 'class_weights':\n",
    "    print(\"Pesos de clase:\", class_weights)\n",
    "\n",
    "# Entrenar modelo\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar métricas de entrenamiento\n",
    "plot_history(history, title=\"Métricas de Entrenamiento - LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar en conjunto de prueba\n",
    "metrics = evaluate_model(model, X_test, y_test, show_confusion_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener reporte de clasificación detallado\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int).flatten()\n",
    "print(classification_report(y_test, y_pred, target_names=['Negativo', 'Positivo']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Guardar Modelo y Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio para modelos si no existe\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Guardar modelo\n",
    "model_path = '../models/lstm_model.h5'\n",
    "model.save(model_path)\n",
    "print(f\"Modelo guardado en: {model_path}\")\n",
    "\n",
    "# Guardar tokenizer\n",
    "tokenizer_path = '../models/lstm_tokenizer.json'\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open(tokenizer_path, 'w') as f:\n",
    "    f.write(tokenizer_json)\n",
    "print(f\"Tokenizer guardado en: {tokenizer_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predicción con Ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, max_len=MAX_LEN):\n",
    "    \"\"\"\n",
    "    Predice el sentimiento de un texto usando el modelo entrenado\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto para predecir\n",
    "        model: Modelo entrenado\n",
    "        tokenizer: Tokenizer ajustado a los datos de entrenamiento\n",
    "        max_len (int): Longitud máxima de secuencia\n",
    "        \n",
    "    Returns:\n",
    "        dict: Predicción con detalles\n",
    "    \"\"\"\n",
    "    # Limpiar el texto\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Convertir a secuencia\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    \n",
    "    # Padding\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
    "    \n",
    "    # Predecir\n",
    "    prediction = model.predict(padded_sequence)[0][0]\n",
    "    \n",
    "    # Determinar sentimiento\n",
    "    sentiment = \"positivo\" if prediction >= 0.5 else \"negativo\"\n",
    "    confidence = prediction if prediction >= 0.5 else 1 - prediction\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'cleaned_text': cleaned_text,\n",
    "        'sentiment': sentiment,\n",
    "        'confidence': float(confidence),\n",
    "        'raw_score': float(prediction)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplos de tweets para probar\n",
    "example_tweets = [\n",
    "    \"I absolutely love this new phone! It's amazing and works perfectly.\",\n",
    "    \"This movie was terrible. Complete waste of time and money.\",\n",
    "    \"The weather today is okay, nothing special.\",\n",
    "    \"Can't believe how bad the customer service was. Never going back!\",\n",
    "    \"Just had the best meal of my life at that new restaurant downtown!\"\n",
    "]\n",
    "\n",
    "# Realizar predicciones\n",
    "results = []\n",
    "for tweet in example_tweets:\n",
    "    result = predict_sentiment(tweet, model, tokenizer)\n",
    "    results.append(result)\n",
    "\n",
    "# Mostrar resultados\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nEjemplo {i+1}:\")\n",
    "    print(f\"Texto: {result['text']}\")\n",
    "    print(f\"Sentimiento: {result['sentiment']}\")\n",
    "    print(f\"Confianza: {result['confidence']:.4f}\")\n",
    "    print(f\"Puntuación: {result['raw_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparación con el Modelo RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intentar cargar el modelo RNN previamente entrenado\n",
    "try:\n",
    "    rnn_model = tf.keras.models.load_model('../models/rnn_model.h5')\n",
    "    \n",
    "    # Evaluar modelo RNN\n",
    "    print(\"Evaluación del modelo RNN:\")\n",
    "    rnn_metrics = evaluate_model(rnn_model, X_test, y_test, show_confusion_matrix=False)\n",
    "    \n",
    "    # Evaluar modelo LSTM\n",
    "    print(\"\\nEvaluación del modelo LSTM:\")\n",
    "    lstm_metrics = evaluate_model(model, X_test, y_test, show_confusion_matrix=False)\n",
    "    \n",
    "    # Crear tabla comparativa\n",
    "    comparison = pd.DataFrame({\n",
    "        'RNN': [rnn_metrics['accuracy'], rnn_metrics['precision'], rnn_metrics['recall'], rnn_metrics['f1']],\n",
    "        'LSTM': [lstm_metrics['accuracy'], lstm_metrics['precision'], lstm_metrics['recall'], lstm_metrics['f1']]\n",
    "    }, index=['Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "    \n",
    "    # Visualizar comparación\n",
    "    print(\"\\nComparación de modelos:\")\n",
    "    print(comparison)\n",
    "    \n",
    "    # Graficar comparación\n",
    "    comparison.plot(kind='bar', figsize=(10, 6))\n",
    "    plt.title('Comparación de Modelos: RNN vs LSTM')\n",
    "    plt.ylabel('Valor')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"No se pudo cargar el modelo RNN para comparación: {e}\")\n",
    "    print(\"Entrene primero el modelo RNN usando el notebook 02_entrenamiento_RNN.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusiones sobre el Modelo LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook, hemos implementado y entrenado un modelo LSTM para la clasificación de sentimientos en tweets. Algunas observaciones importantes:\n",
    "\n",
    "1. **Arquitectura**: El modelo LSTM está diseñado para capturar dependencias a largo plazo en secuencias, lo que lo hace especialmente adecuado para tareas de procesamiento de lenguaje natural como el análisis de sentimiento.\n",
    "\n",
    "2. **Métricas de rendimiento**: El modelo LSTM alcanzó un accuracy de X.XX en el conjunto de prueba, con una precisión de X.XX y un recall de X.XX. (Estos valores se completarán después del entrenamiento)\n",
    "\n",
    "3. **Comparación con RNN**: \n",
    "   - El LSTM generalmente muestra un mejor rendimiento que la RNN básica, especialmente en términos de recall para la clase minoritaria (tweets positivos).\n",
    "   - La capacidad del LSTM para mantener información a lo largo de secuencias más largas ayuda a capturar mejor el contexto de los tweets.\n",
    "\n",
    "4. **Manejo del desbalanceo**: La estrategia de oversampling sigue siendo efectiva con LSTM, permitiendo un aprendizaje más equilibrado entre las clases.\n",
    "\n",
    "5. **Próximos pasos**: Implementaremos un modelo BiLSTM con mecanismo de atención para mejorar aún más el rendimiento, especialmente en la identificación de las partes más relevantes de los tweets para la clasificación de sentimiento.\n",
    "\n",
    "El modelo LSTM proporciona una mejora sobre la RNN básica, demostrando la importancia de utilizar arquitecturas que puedan manejar mejor las dependencias a largo plazo en textos como los tweets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
